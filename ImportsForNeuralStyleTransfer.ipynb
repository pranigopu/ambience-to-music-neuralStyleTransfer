{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98202bf1-f20f-477a-881e-37c7ecf7815a",
   "metadata": {},
   "source": [
    "# IMPORTS FOR NEURAL STYLE TRANSFER\n",
    "Drawing from the model building functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283abb23-c782-4643-8c19-908cab44e35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from genreClassification\\ImportsForAudioHandling.ipynb\n",
      "importing Jupyter notebook from genreClassification\\ImportsForModelHandling.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "N = import_ipynb.NotebookLoader(path=['genreClassification'])\n",
    "N.load_module(\"ImportsForAudioHandling\")\n",
    "N.load_module(\"ImportsForModelHandling\")\n",
    "from ImportsForAudioHandling import *\n",
    "from ImportsForModelHandling import *\n",
    "\n",
    "# For handling tensors:\n",
    "import tensorflow as tf\n",
    "\n",
    "# For handling graphical display:\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f6647-5e1f-4d4f-b1a6-ded1e427263d",
   "metadata": {},
   "source": [
    "# Miscellaneous helper function(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7bbef-104e-4108-ae97-93534b0369ad",
   "metadata": {},
   "source": [
    "Function to show and return all content and style audio names separately based on given tags..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256ec5c-593f-46aa-9f36-e468fae73941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_file_references(path='AUDIO', return_value=False):\n",
    "    all_file_names = get_file_names(path)\n",
    "    # NOTE: `get_file_names` defined in `genreClassification/ImportsForAudioHandling.ipynb`\n",
    "    \n",
    "    # Segregating audio based on content and style tags:\n",
    "    all_content_names, all_style_names = [], []\n",
    "    for file_name in all_file_names:\n",
    "        if 'CONTENT' in file_name:\n",
    "            all_content_names.append(file_name)  \n",
    "        elif 'STYLE' in file_name:\n",
    "            all_style_names.append(file_name)  \n",
    "    \n",
    "    print('ID\\t | Content name\\n------------------------------------------------')\n",
    "    for i, j in enumerate(all_content_names):\n",
    "        print(f'{i}\\t | {j}')\n",
    "    print('\\n\\nID\\t | Style name\\n------------------------------------------------')\n",
    "    for i, j in enumerate(all_style_names):\n",
    "        print(f'{i}\\t | {j}')\n",
    "\n",
    "    if return_value:\n",
    "        return all_content_names, all_style_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628a6b3-8de4-435a-addb-8e8e0b0f5d60",
   "metadata": {},
   "source": [
    "# Audio handling\n",
    "\n",
    "**IMPLEMENTATION NOTE**: To make function calls using customisable parameters easier, I designed the functions such that all audio parameters that may be relevant are assigned to an object that can access the relevant functions. Extra data may be passed in the argument in necessary, but such a design makes it very convenient to test for different audio parameters and makes the code less messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11819129-9dab-44d8-b91e-f5e81dbe3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataHandler:\n",
    "    def __init__(self, content_path, style_path, sr, n_fft, hop_length, segment_size, n_mels, n_mfcc):\n",
    "        # Audio path parameters:\n",
    "        self.content_path = content_path\n",
    "        self.style_path = style_path\n",
    "\n",
    "        # Audio parameters:\n",
    "        self.sr = sr                     # Sampling rate\n",
    "        self.n_fft = n_fft               # FFT window size\n",
    "        self.hop_length = hop_length     # Hop length\n",
    "        self.segment_size = segment_size # Number of frames per melspectrogram segment\n",
    "        self.n_mels = n_mels             # Number of mel bands\n",
    "        self.n_mfcc = n_mfcc             # Number of MFCCs per audio file\n",
    "\n",
    "    #================================================\n",
    "    # PARAMETER DISPLAY\n",
    "\n",
    "    def display_params(self):\n",
    "        for key in vars(self):\n",
    "            print(f'{key}: {vars(self)[key]}')\n",
    "\n",
    "    #############################################################\n",
    "    # Basic audio-handling functions...\n",
    "\n",
    "    #================================================\n",
    "    # Function to obtain time-domain signal:\n",
    "    def get_signal(self, audio_path, audio_name):\n",
    "        # NOTE: Try-except is for increasing robustness of file path handling\n",
    "        try:\n",
    "            signal, _ = librosa.load(audio_path + '/' + audio_name, sr=self.sr)\n",
    "        except:\n",
    "            signal, _ = librosa.load(audio_path + audio_name, sr=self.sr)\n",
    "        return signal\n",
    "\n",
    "    #================================================\n",
    "    # Function to obtain melspectrogram:\n",
    "    def get_melspectrogram(self, signal=None, audio_path=None, audio_name=None):\n",
    "        # Get signal if not given:\n",
    "        if signal is None:\n",
    "            signal = self.get_signal(audio_name)\n",
    "\n",
    "        # Get melspectrogram:\n",
    "        melspectrogram = librosa.feature.melspectrogram(y=signal, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop_length, n_mels=self.n_mels)\n",
    "        return melspectrogram\n",
    "\n",
    "    #================================================\n",
    "    # Function to obtain MFCCs:\n",
    "    def get_mfccs(self, signal=None, audio_path=None, audio_name=None):\n",
    "        # Get signal if not given:\n",
    "        if signal is None:\n",
    "            signal = self.get_signal(audio_path, audio_name)\n",
    "\n",
    "        # Get MFCCs:\n",
    "        mfccs = librosa.feature.mfcc(y=signal, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop_length, n_mfcc=self.n_mfcc)\n",
    "        return mfccs\n",
    "\n",
    "    #================================================\n",
    "    # Function to convert melspectrogram to MFCCs:\n",
    "    # NOTE: The MFCC calculation will depend on the peak loudness in decibels\n",
    "    def melspectrogram_to_mfccs(self, melspectrogram):\n",
    "        # NOTE: `melspectrogram` is not intended to be log-power (i.e. decibel-scaled), only power\n",
    "\n",
    "        # Log-power melspectrogram:\n",
    "        if len(melspectrogram.shape) == 3:\n",
    "            melspectrogram = tf.math.log(tf.transpose(melspectrogram + 1e-6, perm=(0, 2, 1)))\n",
    "        else:\n",
    "            melspectrogram = tf.math.log(tf.transpose(melspectrogram + 1e-6, perm=(1, 0)))\n",
    "\n",
    "        # Calculating MFCCs on log-power melspectrogram:\n",
    "        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(melspectrogram)\n",
    "\n",
    "        # Transposing (for convenience) then truncating as specified:\n",
    "        if len(mfccs.shape) == 3:\n",
    "            mfccs = tf.transpose(mfccs, perm=(0, 2, 1))[:, :self.n_mfcc, :]\n",
    "        else:\n",
    "            mfccs = tf.transpose(mfccs, perm=(1, 0))[:self.n_mfcc, :]\n",
    "\n",
    "        return mfccs\n",
    "\n",
    "    #================================================\n",
    "    # Function to convert MFCCs to melspectrogram:\n",
    "    # NOTE: The MFCC calculation will depend on the peak loudness in decibels\n",
    "    def mfccs_to_melspectrogram(self, mfccs):\n",
    "        # Calculating MFCCs:\n",
    "        melspectrogram = librosa.feature.inverse.mfcc_to_mel(mfcc=np.array(mfccs), n_mels=self.n_mels)\n",
    "\n",
    "        # Converting log scale to linear scale:\n",
    "        melspectrogram = librosa.db_to_power(melspectrogram)\n",
    "        # NOTE: `melspectrogram` is not intended to be log-power (i.e. decibel-scaled), only power\n",
    "        return melspectrogram\n",
    "\n",
    "    #================================================\n",
    "    # Function to play audio signal:\n",
    "    def play_signal(self, signal, sr=None):\n",
    "        if sr is None:\n",
    "            sr = self.sr\n",
    "\n",
    "        audio_element_url = ipd.Audio(signal, rate=sr)\n",
    "        ipd.display(audio_element_url)\n",
    "\n",
    "    #================================================\n",
    "    # Small wrapper function to play audio file by first obtaining the audio file's signal:\n",
    "    def play_audio_file(self, audio_path, audio_file, sr=None):\n",
    "        if sr is None:\n",
    "            sr = self.sr\n",
    "\n",
    "        signal = self.get_signal(audio_path, audio_file, sr=sr)\n",
    "        audio_element_url = ipd.Audio(signal, rate=sr)\n",
    "        ipd.display(audio_element_url)\n",
    "\n",
    "    #================================================\n",
    "    # Small wrapper function to display the waveform of a given signal and then play it:\n",
    "    def display_and_play_signal(self, signal, sr=None, title=None):\n",
    "        if sr is None:\n",
    "            sr = self.sr\n",
    "\n",
    "        plt.plot(signal)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "        self.play_signal(signal, sr=sr)\n",
    "\n",
    "    #================================================\n",
    "    # Function to visualise audio data:\n",
    "    def visualise(self, spectrogram, title=None, xlabel='frame', ylabel=None):\n",
    "        # NOTE: Audio may be visualised as either a spectrogram (including melspectrogram) or MFCCs\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(spectrogram, aspect='auto', origin='lower')\n",
    "        fig.colorbar(img)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "        # NOTE: The colorbar that indicates amplitude\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    #############################################################\n",
    "    # Segmentation and de-segmentation of melspectrograms...\n",
    "\n",
    "    #================================================\n",
    "    # The segmentation function that represents the full preprocessing pipeline for audio:\n",
    "    def get_segments(self, content_name=None, style_name=None, return_signals=False, specifications=''):\n",
    "        # NOTE: Segment size in `audio_params` is given in number of frames\n",
    "\n",
    "        # Getting signals...\n",
    "        content_signal = self.get_signal(self.content_path, content_name)\n",
    "        style_signal = self.get_signal(self.style_path, style_name)\n",
    "\n",
    "        # Getting the audio data (melspectrograms or MFCCs, as specified):\n",
    "        if 'content mfcc' in specifications:\n",
    "            content_data = self.get_mfccs(signal=self.content_signal)\n",
    "        else: # Default to gathering melspectrograms\n",
    "            content_data = self.get_melspectrogram(signal=content_signal)\n",
    "        if 'style mfcc' in specifications:\n",
    "            style_data = self.get_mfccs(signal=style_signal)\n",
    "        else: # Default to gathering melspectrograms\n",
    "            style_data = self.get_melspectrogram(signal=style_signal)\n",
    "\n",
    "        #------------------------------------\n",
    "        # Getting segments...\n",
    "\n",
    "        segment_size = self.segment_size # Renaming for convenience\n",
    "\n",
    "        # Content segments:\n",
    "        content_segments = []\n",
    "        for i in range(int(content_data.shape[1]/segment_size)):\n",
    "            content_segments.append(content_data[:, i*segment_size:(i+1)*segment_size])\n",
    "        content_segments = np.array(content_segments) # Converting to an array for ease of handling and storing\n",
    "\n",
    "        # Style segments:\n",
    "        style_segments = []\n",
    "        for i in range(int(style_data.shape[1]/segment_size)):\n",
    "            style_segments.append(style_data[:, i*segment_size:(i+1)*segment_size])\n",
    "        style_segments = np.array(style_segments) # Converting to an array for ease of handling and storing\n",
    "\n",
    "        if return_signals:\n",
    "            return content_segments, style_segments, content_signal, style_signal\n",
    "        return content_segments, style_segments\n",
    "\n",
    "    #================================================\n",
    "    # De-segmentation, i.e. stiching segments together...\n",
    "    def stitch_segments(self, segments):\n",
    "        return np.concatenate(list(segments), axis=1)\n",
    "\n",
    "    #############################################################\n",
    "    # Signal reconstruction from melspectrogram(s)...\n",
    "\n",
    "    #================================================\n",
    "    def reconstruct_signal_from_melspectrogram(self, melspectrogram):\n",
    "        reconstructed_signal = librosa.feature.inverse.mel_to_audio(melspectrogram, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        return reconstructed_signal\n",
    "\n",
    "    #================================================\n",
    "    def reconstruct_signal_from_melspectrogram_segments(self, segments):\n",
    "        melspectrogram = self.stitch_segments(segments)\n",
    "        return self.reconstruct_signal_from_melspectrogram(melspectrogram)\n",
    "\n",
    "    #================================================\n",
    "    # Small wrapper that enables playing signal reconstructed from melspectrogram:\n",
    "    def reconstruct_signal_from_melspectrogram_and_play(self, melspectrogram, return_value=True):\n",
    "        reconstructed_signal = self.reconstruct_signal_from_melspectrogram(melspectrogram)\n",
    "        audio_element_url = ipd.Audio(reconstructed_signal, rate=self.sr)\n",
    "        ipd.display(audio_element_url)\n",
    "        if return_value:\n",
    "            return reconstructed_signal\n",
    "\n",
    "    #================================================\n",
    "    # Small wrapper that enables playing signal reconstructed from melspectrogram segments:\n",
    "    def reconstruct_signal_from_melspectrogram_segments_and_play(self, segments, return_value=True):\n",
    "        reconstructed_signal = self.reconstruct_signal_from_melspectrogram_segments(segments)\n",
    "        audio_element_url = ipd.Audio(reconstructed_signal, rate=self.sr)\n",
    "        ipd.display(audio_element_url)\n",
    "        if return_value:\n",
    "            return reconstructed_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31664286-3e3b-4c8f-8b3d-ec27f9700e90",
   "metadata": {},
   "source": [
    "**NOTE ON** `tensorflow.signal.mfccs_from_mel_spectrograms`:\n",
    "\n",
    "MFCCs are returned for every mel band in the melspectrogram, and it is up to the caller to select a subset of the MFCCs based on their application. For example, it is typical to only use the first few for speech recognition, as this results in an approximately pitch-invariant representation of the signal. For my purposes, I shall be using the first 40 MFCCs.\n",
    "\n",
    "> REFERENCE: https://docs.w3cub.com/tensorflow~1.15/signal/mfccs_from_log_mel_spectrograms.html\n",
    "\n",
    "---\n",
    "\n",
    "**ALTERNATE CODE FOR MELSPECTROGRAM TO MFCCS USING** `librosa`:\n",
    "\n",
    "```python\n",
    "def melspectrogram_to_mfccs(self, melspectrogram):\n",
    "    # NOTE: `melspectrogram` is not intended to be log-power (i.e. decibel-scaled), only power\n",
    "    \n",
    "    # Log-power melspectrogram:\n",
    "    melspectrogram = librosa.power_to_db(melspectrogram)\n",
    "\n",
    "    # Calculating MFCCs:\n",
    "    mfccs = librosa.feature.mfcc(S=melspectrogram, n_mfcc=self.n_mfcc)\n",
    "    return mfccs\n",
    "```\n",
    "\n",
    "I have used the Tensorflow version since it allows for gradient calculation of Tensorflow variables, which I needed for neural style transfer using unever modelling which involved using the MFCCs of a target melspectrogram to evaluate style cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ca59b-7bc0-4748-b88e-3fe33542f245",
   "metadata": {},
   "source": [
    "# Neural style transfer (NST)\n",
    "For explanations of each of the NST-specific functions, check `DemoForNeuralStyleTransfer.ipynb`. I have used a class to encapsulate the primary NST functionalities to ensure abstraction and hence greater convenience during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30f9459-7b3a-4648-b618-17e9939517cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NST:\n",
    "    def __init__(self, content_layers, style_layers, content_weight, style_weight, model=None, content_model=None, style_model=None):\n",
    "        self.content_weight = content_weight\n",
    "        self.style_weight = style_weight\n",
    "\n",
    "        #------------------------------------\n",
    "        # Obtaining feature extractor(s) along with content and style layers...\n",
    "\n",
    "        if model is None and content_model is None and style_model is None:\n",
    "            print('Model for either content or style is not given!')\n",
    "\n",
    "        try: # Default\n",
    "            self.feature_extractor, layer_labels = self.get_feature_extractor(model)\n",
    "            self.content_layers = [layer_labels[i] for i in content_layers]\n",
    "            self.style_layers = [layer_labels[i] for i in style_layers]\n",
    "        except: #Special case\n",
    "            self.content_feature_extractor, layer_labels_1 = self.get_feature_extractor(content_model)\n",
    "            self.style_feature_extractor, layer_labels_2 = self.get_feature_extractor(style_model)\n",
    "            self.content_layers = [layer_labels_1[i] for i in content_layers]\n",
    "            self.style_layers = [layer_labels_2[i] for i in style_layers]\n",
    "\n",
    "    #================================================\n",
    "    # PARAMETER DISPLAY\n",
    "\n",
    "    def display_params(self):\n",
    "        for key in vars(self):\n",
    "            print(f'{key}: {vars(self)[key]}')\n",
    "\n",
    "    #================================================\n",
    "    # FEATURE EXTRACTOR\n",
    "\n",
    "    def get_feature_extractor(self, model, return_layer_labels=True):\n",
    "        outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "        feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n",
    "\n",
    "        if return_layer_labels:\n",
    "            return feature_extractor, list(outputs_dict.keys())\n",
    "        return feature_extractor\n",
    "\n",
    "    #================================================\n",
    "    # DATA PACKAGING FOR NST\n",
    "\n",
    "    def get_data_for_nst(self, content_segments, style_segments, target_type=None, seed=None):\n",
    "        # `seed` is the pRNG seed for initialising target array:\n",
    "\n",
    "        # Reshaping `content_segments` and `style_segments` to fit the desired model's layers' input shape:\n",
    "        # NOTE 1: Reshaping here just involves appending an extra dimension of magnitude 1 (corresponding to the \"channel\" dimension in the CNN's layers)\n",
    "        # NOTE 2: Such reshaping was also done when supplying training data to the model during training\n",
    "        content_segments = np.reshape(content_segments, newshape=list(content_segments.shape) + [1])\n",
    "        style_segments = np.reshape(style_segments, newshape=list(style_segments.shape) + [1])\n",
    "\n",
    "        # Get feature vectors representations for audio file:\n",
    "        content = tf.Variable(content_segments)\n",
    "        style = tf.Variable(style_segments)\n",
    "        # NOTE: Renaming is done only for readability\n",
    "        if target_type is None or target_type == 'random':\n",
    "            target = tf.Variable(tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=seed)(shape=content_segments.shape))\n",
    "        elif target_type == 'zero':\n",
    "            target = tf.Variable(tf.zeros(shape=content_segments.shape))\n",
    "        elif target_type == 'content':\n",
    "            target = tf.Variable(content_segments)\n",
    "        elif target_type == 'style':\n",
    "            target = tf.Variable(style_segments)\n",
    "\n",
    "        return content, style, target\n",
    "\n",
    "    #================================================\n",
    "    # GRAM MATRIX CALCULATION\n",
    "\n",
    "    def get_gram_matrix(self, layer_outputs):\n",
    "        # 1. Flatten each layer output to obtain the feature vectors:\n",
    "        layer_outputs = tf.transpose(layer_outputs, (3, 0, 1, 2)) # What is this for? See the text box below this code box\n",
    "        layer_outputs = tf.reshape(layer_outputs, (tf.shape(layer_outputs)[0], -1))\n",
    "\n",
    "        # 2. Defining a tensor A containing each feature vector:\n",
    "        # We already have `layer_outputs`\n",
    "\n",
    "        # 3. Define a tensor B containing the transpose of each feature vector (in matching order):\n",
    "        '''\n",
    "        For this, we can simply do `tf.transpose(layer_outputs)`.\n",
    "        Such an application of `tf.transpose` will reverse the dimensionality of `layer_outputs`.\n",
    "        This not only transposes the outer tensor but every feature vector within as well.\n",
    "        Hence, this obtains B^T.\n",
    "        '''\n",
    "\n",
    "        # 4. Obtain AB^T (see previous comment):\n",
    "        gram_matrix = tf.matmul(layer_outputs, tf.transpose(layer_outputs))\n",
    "        # NOTE: `tf.matmul` performs matrix multiplication\n",
    "        return gram_matrix\n",
    "\n",
    "    #================================================\n",
    "    # CONTENT LOSS CALCULATION\n",
    "\n",
    "    def get_content_loss(self, layer_output_for_content, layer_output_for_target):\n",
    "        n1, n2, n3, n4 = layer_output_for_target.shape\n",
    "\n",
    "        # Returning sum of squared errors between layer outputs:\n",
    "        return tf.reduce_sum(tf.square(layer_output_for_content-layer_output_for_target))/(n1*n2*n3*n4)\n",
    "        # NOTE: We want to preserve the `tf.tensor` datatype\n",
    "\n",
    "    #================================================\n",
    "    # STYLE LOSS CALCULATION\n",
    "\n",
    "    def get_style_loss(self, layer_output_for_style, layer_output_for_target):\n",
    "        n1, n2, n3, n4 = layer_output_for_target.shape\n",
    "\n",
    "        # Obtaining Gram matrices:\n",
    "        style_gram_matrix = self.get_gram_matrix(layer_output_for_style) # Gram matrix for the style image\n",
    "        target_gram_matrix = self.get_gram_matrix(layer_output_for_target) # Gram matrix for the target image\n",
    "\n",
    "        # Returning a constant multiple of mean-squared error:\n",
    "        return tf.reduce_sum(tf.square(style_gram_matrix-target_gram_matrix))/(n1*n2*n3*n4)\n",
    "        # NOTE 1: `tf.reduce_sum` Computes the sum of elements across dimensions of a tensor\n",
    "        # NOTE 2: We want to preserve the `tf.tensor` datatype, hence we apply operations from Tensorflow and not NumPy\n",
    "\n",
    "    #================================================\n",
    "    # TOTAL LOSS CALCULATION\n",
    "\n",
    "    def get_total_loss(self, content, style, target):\n",
    "        # Getting content, style and target features:\n",
    "        content_features = self.feature_extractor(content)\n",
    "        style_features = self.feature_extractor(style)\n",
    "        target_features = self.feature_extractor(target)\n",
    "\n",
    "        # Initialising loss value:\n",
    "        loss = tf.zeros(shape=())\n",
    "\n",
    "        for layer in self.content_layers:\n",
    "            layer_output_for_content = content_features[layer]\n",
    "            layer_output_for_target = target_features[layer]\n",
    "            loss += self.content_weight * self.get_content_loss(layer_output_for_content, layer_output_for_target)\n",
    "\n",
    "        for layer in self.style_layers:\n",
    "            layer_output_for_style = style_features[layer]\n",
    "            layer_output_for_target = target_features[layer]\n",
    "            loss += self.style_weight * self.get_style_loss(layer_output_for_style, layer_output_for_target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    #================================================\n",
    "    # GRADIENT AND TOTAL LOSS CALCULATION\n",
    "\n",
    "    def get_loss_and_grads(self, content, style, target):\n",
    "        # NOTE: Each of target, content and style are arrays of segments of the melspectrogram of a particular audio file\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.get_total_loss(content, style, target)\n",
    "        # Obtaining the gradient of loss w.r.t. `target`:\n",
    "        grads = tape.gradient(loss, target)\n",
    "        return loss, grads\n",
    "\n",
    "    #================================================\n",
    "    # BASIC NST LOOP\n",
    "\n",
    "    def nst_loop(self, content, style, target, n_iter, initial_learning_rate=0.01, decay_steps=10, decay_rate=0.8, print_frequency=10):\n",
    "        optimizer = keras.optimizers.SGD(tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=initial_learning_rate, decay_steps=decay_steps, decay_rate=decay_rate))\n",
    "        # NOTE: Reassigning the optimizer resets its gradient\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            # Calculating total loss and gradient:\n",
    "            loss, grads = self.get_loss_and_grads(content, style, target)\n",
    "\n",
    "            # Applying gradients:\n",
    "            optimizer.apply_gradients([(grads, target)])\n",
    "\n",
    "            # Displaying process:\n",
    "            if i % print_frequency == 0 or (i == n_iter - 1):\n",
    "                #visualise(stitch_segments(target))\n",
    "                print(f'i={i}\\t | loss={float(loss):.5e} \\t | lr={float(optimizer.learning_rate.numpy()):.5e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d0065-03ce-4abd-bc10-a5abc1229b69",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION NOTE: Effect of reshaping by appending an extra dimension of magnitude 1**:\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "```python\n",
    "# Original array:\n",
    "a = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "print('Original array:')\n",
    "print(a.shape)\n",
    "print(a)\n",
    "# Reshaped array\n",
    "print('\\nReshaped array:')\n",
    "b = np.reshape(a, newshape=list(a.shape) + [1])\n",
    "print(b.shape)\n",
    "print(b)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Original array:\n",
    "[[1, 2, 3, 4],\n",
    " [5, 6, 7, 8]]\n",
    "(2, 4)\n",
    "\n",
    "Reshaped array:\n",
    "[[[1],\n",
    "  [2],\n",
    "  [3],\n",
    "  [4]],\n",
    " [[5],\n",
    "  [6],\n",
    "  [7],\n",
    "  [8]]]\n",
    "(2, 4, 1)\n",
    "```\n",
    "\n",
    "We observe that we convert an array of row vectors to an array of column vectors with the same values, i.e. both are of the form `[v1, v2]`, where each of `v1` and `v2` are vectors with a constant set of values; in the first case they are row vectors, in the second (reshaped) case they are column vectors. Hence, we see that the 4th dimension in the CNN model's layers, i.e. the channel dimension, is such that each channel is a separate column in the last dimension; in this way, the same set of inputs is handled across multiple channels (multiple filters, in convolutional layers) within the same 4-dimensional matrix. Very neat!\n",
    "\n",
    "**NOTE**: Such reshaping was also done when supplying training data to the model during training\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
