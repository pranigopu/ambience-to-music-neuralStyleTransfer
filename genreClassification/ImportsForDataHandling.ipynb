{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C8QHrXWfcDp"
   },
   "source": [
    "# IMPORTS FOR DATA HANDLING\n",
    "**NOTE**: The functions below are designed to be easy to import and thus independent of specific user-defined files. Hence, there are constants (such as the list of genres or number of genres) that were obtained elsewhere but are simply typed out here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported libraries & modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For handling data:\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# For enabling data shuffling:\n",
    "from random import shuffle\n",
    "\n",
    "# For handling tensors:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing datasets\n",
    "Preparing data for the following:\n",
    "\n",
    "- Viewing and working the data and target labels in simple formats\n",
    "- Working with neural networks (abstracting aspects like batches and data shuffling)\n",
    "\n",
    "**SOME NOTES**:\n",
    "\n",
    "- `to_categorical` was imported as `from tensorflow.keras.utils import to_categorical`\n",
    "- `to_categorical` converts integer labels to the appropriate 1-hot encoding\n",
    "- The below is mostly to increases convenience; we can do without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenting the data (along with duplicating the corresponding labels of course):\n",
    "def get_segmented_data(df, audio_data, segments_per_file, shuffle_data=True):\n",
    "    # Segmenting the audio data's frames as indicated by `segments_per_file`...\n",
    "    \n",
    "    # NOTE: We assume the audio data to be either spectrograms, melspectrograms or MFCC arrays\n",
    "    # NOTE: We also assume that each entry in the raw data is equally dimensioned\n",
    "    n_frames = audio_data.shape[2] # We assume the data to be shaped as: <Entries>, <MFCCs/Frequencies>, <Frames>\n",
    "    segment_size = int(n_frames // segments_per_file)\n",
    "    data = []\n",
    "    for i in range(len(df['TRACK'])):\n",
    "        for j in range(segments_per_file):\n",
    "            data.append(audio_data[i, :, j*segment_size:(j+1)*segment_size])\n",
    "    \n",
    "    #________________________\n",
    "    # Duplicating labels to match each segment...\n",
    "    # Total labels:\n",
    "    labels = []\n",
    "    for label in df['TARGET']:\n",
    "        labels += [label]*segments_per_file\n",
    "\n",
    "    #________________________\n",
    "    # Shuffling the data for unbiased training and testing (hence better convergence of model):\n",
    "    # Joining melspectrograms and labels to shuffle data and labels in corresponding order...\n",
    "    D = list(zip(data, labels))\n",
    "    # Shuffling list items...\n",
    "    if shuffle_data:\n",
    "        shuffle(D)\n",
    "    # Separating melspectograms and their labels for future convenience...\n",
    "    segmented_data = np.array([d[0] for d in D])\n",
    "    segmented_labels = np.array([d[1] for d in D])\n",
    "\n",
    "    return segmented_data, segmented_labels\n",
    "\n",
    "#================================================\n",
    "# Dividing the data and labels into training and validation datasets:\n",
    "def get_data_in_splits(data, labels, validation_start):\n",
    "    # Specifying proportions for datasets:\n",
    "    validation_start = round(validation_start*len(labels)) # Might as well be `len(data)`\n",
    "    \n",
    "    # Training data:\n",
    "    train_data = data[:validation_start] # Feature values\n",
    "    train_labels = labels[:validation_start] # Target values\n",
    "    \n",
    "    # Testing data:\n",
    "    validation_data = data[validation_start:] # Feature values\n",
    "    validation_labels = labels[validation_start:] # Target values\n",
    "    \n",
    "    print(f'Training data shape = {train_data.shape}, Validation data shape = {validation_data.shape}')\n",
    "\n",
    "    return train_data, train_labels, validation_data, validation_labels\n",
    "\n",
    "#================================================\n",
    "# Get datasets wrapped in a `tf.data.Dataset` object for convenience when working with neural networks:\n",
    "def get_data(df, audio_data, n_classes, segments_per_file=4, validation_start=0.7, batch_size=32, shuffle_data=True):\n",
    "    # NOTE: `n_classes` = Number of target classes\n",
    "    \n",
    "    data, labels = get_segmented_data(df, audio_data, segments_per_file, shuffle_data)\n",
    "    \n",
    "    # Dividing the data and labels into training and validation datasets:\n",
    "    train_data, train_labels, validation_data, validation_labels = get_data_in_splits(data, labels, validation_start)\n",
    "    \n",
    "    #------------------------------------\n",
    "    # Dictionary of training and validation data and labels in simpler data types:\n",
    "    data_and_labels = {}\n",
    "    data_and_labels['train_data'] = train_data\n",
    "    data_and_labels['validation_data'] = validation_data\n",
    "    data_and_labels['train_labels'] = train_labels\n",
    "    data_and_labels['validation_labels'] = validation_labels\n",
    "\n",
    "    #------------------------------------\n",
    "    # Preparing the dataset for working in neural networks:\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_data, to_categorical(train_labels, num_classes=n_classes)))\n",
    "    '''\n",
    "    NOTE:\n",
    "    Shuffling rows in training dataset helps in making the model converge in training.\n",
    "    However, this is not necessary in our case since out dataset was already shuffled before.\n",
    "    However, if it were necessary, we would have done it as follows:\n",
    "    \n",
    "    `train_dataset = train_dataset.shuffle(buffer_size=1024)`\n",
    "    '''\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    \n",
    "    # Preparing the testing dataset:\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((validation_data, to_categorical(validation_labels, num_classes=n_classes)))\n",
    "    validation_dataset = validation_dataset.batch(batch_size)\n",
    "\n",
    "    # Parameters:\n",
    "    params = {}\n",
    "    params['segments_per_file'] = segments_per_file\n",
    "    params['validation_start'] = validation_start\n",
    "    params['n_classes'] = n_classes\n",
    "    params['batch_size'] = batch_size\n",
    "\n",
    "    return params, data_and_labels, train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE ON SHUFFLING DATA BEFORE DIVIDING IT**:\n",
    "\n",
    "Shuffling the data before dividing it into training and testing datasets reduced overfitting and improved the model's accuracy (training and validation). Hence, it seems the original dataset's rows were arranged in a certain order with respect to which the model could overfit; shuffling the rows avoids this issue."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
